---
author: Gregory M. Kapfhammer
title: Can doubling experiments characterize worst-case time complexity?
date: '2015-01-01'
date-format: YYYY
categories: [post, database testing, performance evaluation]
description: <em>Can experiments help us understand algorithmic complexity?</em>
---

## Introduction

Understanding the efficiency of algorithms is crucial for software engineers,
especially when dealing with complex systems. One effective way to empirically
determine an algorithm's worst-case time complexity is through doubling
experiments. This blog post introduces a framework for conducting these
experiments, focusing on the domain of search-based test data generation for
relational database schemas. Keep reading to learn more about this technique and
its broader applicability!

## Doubling Experiments

A doubling experiment involves systematically doubling the size of the input to
an algorithm and observing the change in its runtime. This method helps in
empirically determining the algorithm's order of growth, often expressed in
"big-Oh" notation. By measuring the time needed to run the algorithm on inputs
of size \( n \) and \( 2n \), we can draw conclusions about its efficiency.

Here are some of the key Aspects of this paper's presented technique:

- **Doubling Schemas**: The framework systematically doubles the size of a
relational database schema, including tables, columns, and constraints. This
process is non-trivial due to the complex interrelationships within a schema.

- **Automatic Experimentation**: The framework automates the doubling process
and measures the runtime for each input size. It uses a convergence algorithm to
determine when a stable ratio of runtimes is achieved, indicating the worst-case
time complexity.

- **Empirical Analysis**: The framework was applied to various database schemas,
revealing performance trade-offs in search-based test data generation. The
results showed that, in many cases, the time complexity was linear or
linearithmic, while in others, it was quadratic, cubic, or worse.

While this paper specifically applies doubling experiments to the domain of
relational database schemas, the technique is general and can be used for other
algorithms and domains. By systematically increasing the input size and
observing the runtime changes, software engineers can gain valuable insights
into the efficiency of various algorithms.

## Results

The empirical study conducted using this framework focused on nine different
database schemas. The experiments revealed that doubling certain schema
structures, such as UNIQUE, NOT NULL, and CHECK constraints, often resulted in
linear or linearithmic time complexity. However, doubling the number of tables
and columns produced less conclusive results, with many experiments indicating
quadratic or cubic behavior.

## Future Work

Future research will focus on extending the doubling technique to other types of
constraints, such as FOREIGN KEYs, and implementing more realistic ways to
double relational schemas. Additionally, automated parameter tuning will be
explored to optimize the convergence conditions for different execution
environments.

## Conclusion

Doubling experiments provide a powerful and general approach to empirically
determine the worst-case time complexity of algorithms. By applying this
technique to search-based test data generation for relational database schemas,
we have gained valuable insights into performance trade-offs and efficiency.
This method holds promise for broader applications in software engineering and
beyond.

::: {.callout-note appearance="simple"}

## Further Details

If you have any insights or experiences related to this topic, please
[contact](/contact/) me. To stay informed about new developments and blog posts,
consider [subscribing](/support/) to my mailing list.

:::

{{< include /_back-blog.qmd >}}
